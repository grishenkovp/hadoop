{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hadoop"
      ],
      "metadata": {
        "id": "NtAmIWBpFjSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Java"
      ],
      "metadata": {
        "id": "dSQgCnM4Tat9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the installed Java version\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AUgdnK4TdXo",
        "outputId": "db3f578a-d47c-46b5-e0d7-2d80a8faa6f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.17\" 2022-10-18\n",
            "OpenJDK Runtime Environment (build 11.0.17+8-post-Ubuntu-1ubuntu218.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.17+8-post-Ubuntu-1ubuntu218.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing java 8\n",
        "# -q, quiet level 2: no output except for errors\n",
        "#> /dev/null on the end of any command where you want to redirect all the stdout into nothingness\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "poMDQ1h3TrhU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching java version to use as default (choose option 2)\n",
        "!update-alternatives --config java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kRG96xkURIj",
        "outputId": "588b05eb-a854-402b-fd02-a92466f3707a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching javac version to use as default (choose option 2)\n",
        "!update-alternatives --config javac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XcyXlhFUYnb",
        "outputId": "a8429191-626e-4d92-da43-598573f3ef10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative javac (providing /usr/bin/javac).\n",
            "\n",
            "  Selection    Path                                          Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/javac    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching jps version to use as default (choose option 2)\n",
        "!update-alternatives --config jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU1k_XRUUiUm",
        "outputId": "984af436-394a-4d5f-e57c-f7356d408c74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative jps (providing /usr/bin/jps).\n",
            "\n",
            "  Selection    Path                                        Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/jps    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Java default version\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NU7owzEUn42",
        "outputId": "a1b95d9f-ee69-46a1-afc2-795ef18d6a23"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_352\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_352-8u352-ga-1~18.04-b08)\n",
            "OpenJDK 64-Bit Server VM (build 25.352-b08, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wnwvgw7VXk0",
        "outputId": "01c9d1ae-04ae-44f0-c2bf-5116e016d7df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-8-openjdk-amd64/jre/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Hadoop"
      ],
      "metadata": {
        "id": "5UjCxktVZpfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading Hadoop\n",
        "!wget -q  https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz.sha512"
      ],
      "metadata": {
        "id": "6wo42ETWZleK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wsS_m9wMD1kg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe0adce-5728-4bdf-b6f2-70f7b0226155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ca5e12625679ca95b8fd7bb7babc2a8dcb2605979b901df9ad137178718821097b67555115fafc6dbf6bb32b61864ccb6786dbc555e589694a22bf69147780b4\n"
          ]
        }
      ],
      "source": [
        "#File verification\n",
        "!shasum -a512 hadoop-3.3.4.tar.gz | awk '{print $1}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat hadoop-3.3.4.tar.gz.sha512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-DzZxyuEJTE",
        "outputId": "29ef0791-441d-4bb5-b3d3-cf44742f342d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHA512 (hadoop-3.3.4.tar.gz) = ca5e12625679ca95b8fd7bb7babc2a8dcb2605979b901df9ad137178718821097b67555115fafc6dbf6bb32b61864ccb6786dbc555e589694a22bf69147780b4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Untarring the file\n",
        "!tar -xf hadoop-3.3.4.tar.gz"
      ],
      "metadata": {
        "id": "Y8sykATMEn8T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the tar file\n",
        "!rm hadoop-3.3.4.tar.gz\n",
        "!rm hadoop-3.3.4.tar.gz.sha512"
      ],
      "metadata": {
        "id": "7dk_Nym_FSFb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the hadoop files to user/local\n",
        "#-r copy directories recursively\n",
        "!cp -r hadoop-3.3.4/ /usr/local/"
      ],
      "metadata": {
        "id": "Ty5N6WVvboK6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-3.3.4/etc/hadoop directory\n",
        "!ls /usr/local/hadoop-3.3.4/etc/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW9Y5NRybTNS",
        "outputId": "e4a11e6f-db66-4a36-d64b-d169f908baf2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capacity-scheduler.xml\t\t  kms-log4j.properties\n",
            "configuration.xsl\t\t  kms-site.xml\n",
            "container-executor.cfg\t\t  log4j.properties\n",
            "core-site.xml\t\t\t  mapred-env.cmd\n",
            "hadoop-env.cmd\t\t\t  mapred-env.sh\n",
            "hadoop-env.sh\t\t\t  mapred-queues.xml.template\n",
            "hadoop-metrics2.properties\t  mapred-site.xml\n",
            "hadoop-policy.xml\t\t  shellprofile.d\n",
            "hadoop-user-functions.sh.example  ssl-client.xml.example\n",
            "hdfs-rbf-site.xml\t\t  ssl-server.xml.example\n",
            "hdfs-site.xml\t\t\t  user_ec_policies.xml.template\n",
            "httpfs-env.sh\t\t\t  workers\n",
            "httpfs-log4j.properties\t\t  yarn-env.cmd\n",
            "httpfs-site.xml\t\t\t  yarn-env.sh\n",
            "kms-acls.xml\t\t\t  yarnservice-log4j.properties\n",
            "kms-env.sh\t\t\t  yarn-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-env.sh file\n",
        "!cat hadoop-3.3.4/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "HjGgIFBCbgtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding JAVA_HOME directory to hadoop-env.sh file\n",
        "#Removing comments from other items\n",
        "\n",
        "# export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
        "# export HADOOP_HOME=/usr/local/hadoop-3.3.4\n",
        "# export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop"
      ],
      "metadata": {
        "id": "V9zvDBZEcOVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "!/usr/local/hadoop-3.3.4/bin/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW2TTB9fFoKD",
        "outputId": "198af69b-cd76-4958-e503-f726f2c16b09"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing os module\n",
        "import os\n",
        "#Creating environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.4\""
      ],
      "metadata": {
        "id": "zTJnW-0Hd5l5"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File Management"
      ],
      "metadata": {
        "id": "9chcNtqphszT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hdfs dfs -cp  # Copy files from source to destination\n",
        "# hdfs dfs -mv  # Move files from source to destination\n",
        "# hdfs dfs -mkdir /foodir # Create a directory named /foodir\n",
        "# hdfs dfs -rm /foodir/myfile # Delete the file\n",
        "# hdfs dfs -rmdir /foodir # Delete a directory /foodir    \n",
        "# hdfs dfs -rmr /foodir   # Remove a directory named /foodir and content under it recursively   "
      ],
      "metadata": {
        "id": "KVVTkU5dmySe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List Files"
      ],
      "metadata": {
        "id": "-REkBCsQoIwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hdfs dfs -ls -h -R # Recursively list subdirectories with human-readable file sizes."
      ],
      "metadata": {
        "id": "e73bN3gMoNqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload/Download Files"
      ],
      "metadata": {
        "id": "ToE0HnbioYfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -p : Preserves rights and modification times.\n",
        "# -f : Overwrites the destination if it already exists.\n",
        "# hdfs dfs -put -f /home/myfile /hadoop # Copies the file from local file system to HDFS\n",
        "# hdfs dfs -get hadoop/myfile /home # Copies the file from HDFS to local file system"
      ],
      "metadata": {
        "id": "BEX1Jq5VoiON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read/Write Files"
      ],
      "metadata": {
        "id": "0O5iFay_pdsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hdfs dfs -cat /foodir/myfile #View the contents of a file named /foodir/myfile\n",
        "# hdfs dfs -touchz /foodir/myfile  #Create a file named /foodir/myfile"
      ],
      "metadata": {
        "id": "HEgd0iw4g0NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "GgMQwgj8p44Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /content/hadoop_data"
      ],
      "metadata": {
        "id": "oAqcHJHKp6wq"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!find / -type d -name hadoop_data"
      ],
      "metadata": {
        "id": "Z2DRpdY5xqKb"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -put -f /content/drive/MyDrive/datasets/test_data.csv /content/hadoop_data"
      ],
      "metadata": {
        "id": "dm8IAkbD1Yb9"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -ls -h -R /content/hadoop_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSsI1U7gzmp1",
        "outputId": "05529a3c-6434-4ade-a80d-018212288010"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r--   1 root root        380 2023-01-07 11:13 /content/hadoop_data/test_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -cat /content/hadoop_data/test_data.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3limwC18Cj5",
        "outputId": "df87ad85-22c3-4186-a6e2-5e8a7c6a6e18"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "region,manager,product,amount\r\n",
            "r2,m1,pr3,12\r\n",
            "r2,m5,pr6,49\r\n",
            "r2,m3,pr1,49\r\n",
            "r5,m4,pr7,59\r\n",
            "r5,m2,pr2,68\r\n",
            "r1,m4,pr3,50\r\n",
            "r2,m5,pr2,21\r\n",
            "r2,m5,pr1,21\r\n",
            "r6,m4,pr4,68\r\n",
            "r6,m3,pr2,22\r\n",
            "r5,m3,pr2,46\r\n",
            "r5,m2,pr5,70\r\n",
            "r6,m2,pr1,64\r\n",
            "r1,m5,pr3,59\r\n",
            "r4,m3,pr6,66\r\n",
            "r9,m4,pr2,82\r\n",
            "r2,m5,pr1,57\r\n",
            "r1,m1,pr5,46\r\n",
            "r10,m2,pr2,74\r\n",
            "r6,m5,pr3,28\r\n",
            "r2,m2,pr3,85\r\n",
            "r2,m5,pr1,15\r\n",
            "r4,m5,pr5,79\r\n",
            "r4,m2,pr5,51\r\n",
            "r4,m2,pr3,84"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -text /content/hadoop_data/test_data.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBSLolJH8CaR",
        "outputId": "616ab068-9cd8-45ff-f665-08d944c643ab"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "region,manager,product,amount\r\n",
            "r2,m1,pr3,12\r\n",
            "r2,m5,pr6,49\r\n",
            "r2,m3,pr1,49\r\n",
            "r5,m4,pr7,59\r\n",
            "r5,m2,pr2,68\r\n",
            "r1,m4,pr3,50\r\n",
            "r2,m5,pr2,21\r\n",
            "r2,m5,pr1,21\r\n",
            "r6,m4,pr4,68\r\n",
            "r6,m3,pr2,22\r\n",
            "r5,m3,pr2,46\r\n",
            "r5,m2,pr5,70\r\n",
            "r6,m2,pr1,64\r\n",
            "r1,m5,pr3,59\r\n",
            "r4,m3,pr6,66\r\n",
            "r9,m4,pr2,82\r\n",
            "r2,m5,pr1,57\r\n",
            "r1,m1,pr5,46\r\n",
            "r10,m2,pr2,74\r\n",
            "r6,m5,pr3,28\r\n",
            "r2,m2,pr3,85\r\n",
            "r2,m5,pr1,15\r\n",
            "r4,m5,pr5,79\r\n",
            "r4,m2,pr5,51\r\n",
            "r4,m2,pr3,84"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -touchz /content/hadoop_data/test_data2.csv"
      ],
      "metadata": {
        "id": "OO_xtZv48CR5"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -rm -r /content/hadoop_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpczbfPHyG6f",
        "outputId": "aa44bdd3-06bb-4c57-ad26-ded76c411f38"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-07 10:44:05,155 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/hadoop_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -rm /content/hadoop_data/test_data.csv\n",
        "!$HADOOP_HOME/bin/hdfs dfs -rm /content/hadoop_data/test_data2.csv\n",
        "!$HADOOP_HOME/bin/hdfs dfs -rmdir /content/hadoop_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnIOnXkW95Ij",
        "outputId": "cd675f99-db75-470a-d309-f1ab2a9067d8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-07 11:23:02,001 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/hadoop_data/test_data.csv\n",
            "2023-01-07 11:23:03,820 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/hadoop_data/test_data2.csv\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hadoop. Basics"
      ],
      "metadata": {
        "id": "jYu7FK6bxA8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pL31EKHWwaUe"
      },
      "outputs": [],
      "source": [
        "!wget -q  https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n",
        "!tar -xf hadoop-3.3.4.tar.gz\n",
        "!rm hadoop-3.3.4.tar.gz\n",
        "!cp -r hadoop-3.3.4/ /usr/local/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r hadoop-3.3.4"
      ],
      "metadata": {
        "id": "oAyehiEe1uQf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQf_5Mqgxi8o",
        "outputId": "2c1d07d5-05a9-4106-d7e4-20b253368228"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /usr/local/hadoop-3.3.4/etc/hadoop/hadoop-env.sh\n",
        "# export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n",
        "# export HADOOP_HOME=/usr/local/hadoop-3.3.4\n",
        "# export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop"
      ],
      "metadata": {
        "id": "3RMOTPQBxmgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJOsDpRTyj1g",
        "outputId": "c2fca4da-e89e-4fbb-caba-d2345af7f570"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /usr/local/hadoop-3.3.4 -name 'hadoop-streaming*.jar'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SxUMiEd6dJd",
        "outputId": "1e43cce5-0ccf-4a36-e279-afe4aae4e74e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.3.4/share/hadoop/tools/sources/hadoop-streaming-3.3.4-sources.jar\n",
            "/usr/local/hadoop-3.3.4/share/hadoop/tools/sources/hadoop-streaming-3.3.4-test-sources.jar\n",
            "/usr/local/hadoop-3.3.4/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HADOOP_VERSION\"] = \"3.3.4\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.4\"\n",
        "os.environ[\"HADOOP_TOOLS\"] = \"/usr/local/hadoop-3.3.4/share/hadoop/tools/lib\""
      ],
      "metadata": {
        "id": "ET0Ye1huyoeI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /content/mapreduce_hadoop"
      ],
      "metadata": {
        "id": "EwYFGwQf6Yz1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -put -f /content/mapper.py /content/mapreduce_hadoop\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put -f /content/reducer.py /content/mapreduce_hadoop"
      ],
      "metadata": {
        "id": "O7QyPI21-ALM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /content/input_hadoop"
      ],
      "metadata": {
        "id": "KEoW4wHS-0UE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -put -f /content/the_zen_of_python.txt /content/input_hadoop"
      ],
      "metadata": {
        "id": "mBAZq3de_FyD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Папки вывода на момент запуска скрипта не должно существовать!\n",
        "!$HADOOP_HOME/bin/hadoop fs -rm -r /content/output_hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2vT1OIbAaSb",
        "outputId": "d6242e24-ab83-408e-f1de-a515ccbeec3d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-07 16:27:33,369 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output_hadoop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hadoop jar ${HADOOP_TOOLS}/hadoop-streaming-${HADOOP_VERSION}.jar \\\n",
        "    -file  /content/mapreduce_hadoop/mapper.py  -mapper  /content/mapreduce_hadoop/mapper.py \\\n",
        "    -file  /content/mapreduce_hadoop/reducer.py -reducer /content/mapreduce_hadoop/reducer.py \\\n",
        "    -input /content/input_hadoop/*.txt -output /content/output_hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc-_YLb2_hAS",
        "outputId": "d431676b-9068-4022-8228-4b8b71fd3941"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-07 16:27:37,427 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/mapreduce_hadoop/mapper.py, /content/mapreduce_hadoop/reducer.py] [] /tmp/streamjob3267450412070157186.jar tmpDir=null\n",
            "2023-01-07 16:27:38,322 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2023-01-07 16:27:38,562 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2023-01-07 16:27:38,562 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2023-01-07 16:27:38,590 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-01-07 16:27:38,860 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-01-07 16:27:38,887 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2023-01-07 16:27:39,299 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local707423720_0001\n",
            "2023-01-07 16:27:39,299 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-01-07 16:27:39,767 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapreduce_hadoop/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local707423720_0001_eaee49c8-5fde-4d54-b5a2-95a019065675/mapper.py\n",
            "2023-01-07 16:27:39,803 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapreduce_hadoop/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local707423720_0001_355922f4-67ce-450a-bad1-0094999307c0/reducer.py\n",
            "2023-01-07 16:27:39,945 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2023-01-07 16:27:39,948 INFO mapreduce.Job: Running job: job_local707423720_0001\n",
            "2023-01-07 16:27:39,964 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2023-01-07 16:27:39,969 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2023-01-07 16:27:39,976 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-01-07 16:27:39,976 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-01-07 16:27:40,043 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2023-01-07 16:27:40,050 INFO mapred.LocalJobRunner: Starting task: attempt_local707423720_0001_m_000000_0\n",
            "2023-01-07 16:27:40,113 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-01-07 16:27:40,113 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-01-07 16:27:40,180 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-01-07 16:27:40,193 INFO mapred.MapTask: Processing split: file:/content/input_hadoop/the_zen_of_python.txt:0+876\n",
            "2023-01-07 16:27:40,230 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2023-01-07 16:27:40,354 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-01-07 16:27:40,355 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-01-07 16:27:40,355 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-01-07 16:27:40,355 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-01-07 16:27:40,355 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-01-07 16:27:40,367 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-01-07 16:27:40,394 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper.py]\n",
            "2023-01-07 16:27:40,408 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2023-01-07 16:27:40,409 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2023-01-07 16:27:40,410 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2023-01-07 16:27:40,410 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2023-01-07 16:27:40,415 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2023-01-07 16:27:40,415 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2023-01-07 16:27:40,417 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2023-01-07 16:27:40,418 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2023-01-07 16:27:40,418 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2023-01-07 16:27:40,418 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2023-01-07 16:27:40,419 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2023-01-07 16:27:40,420 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2023-01-07 16:27:40,461 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-01-07 16:27:40,462 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-01-07 16:27:40,578 INFO streaming.PipeMapRed: Records R/W=21/1\n",
            "2023-01-07 16:27:40,586 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2023-01-07 16:27:40,589 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2023-01-07 16:27:40,595 INFO mapred.LocalJobRunner: \n",
            "2023-01-07 16:27:40,595 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-01-07 16:27:40,595 INFO mapred.MapTask: Spilling map output\n",
            "2023-01-07 16:27:40,595 INFO mapred.MapTask: bufstart = 0; bufend = 1106; bufvoid = 104857600\n",
            "2023-01-07 16:27:40,595 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213828(104855312); length = 569/6553600\n",
            "2023-01-07 16:27:40,610 INFO mapred.MapTask: Finished spill 0\n",
            "2023-01-07 16:27:40,631 INFO mapred.Task: Task:attempt_local707423720_0001_m_000000_0 is done. And is in the process of committing\n",
            "2023-01-07 16:27:40,637 INFO mapred.LocalJobRunner: Records R/W=21/1\n",
            "2023-01-07 16:27:40,637 INFO mapred.Task: Task 'attempt_local707423720_0001_m_000000_0' done.\n",
            "2023-01-07 16:27:40,653 INFO mapred.Task: Final Counters for attempt_local707423720_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3106\n",
            "\t\tFILE: Number of bytes written=643139\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=21\n",
            "\t\tMap output records=143\n",
            "\t\tMap output bytes=1106\n",
            "\t\tMap output materialized bytes=1398\n",
            "\t\tInput split bytes=100\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=143\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=41\n",
            "\t\tTotal committed heap usage (bytes)=348127232\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=900\n",
            "2023-01-07 16:27:40,653 INFO mapred.LocalJobRunner: Finishing task: attempt_local707423720_0001_m_000000_0\n",
            "2023-01-07 16:27:40,654 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2023-01-07 16:27:40,657 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2023-01-07 16:27:40,658 INFO mapred.LocalJobRunner: Starting task: attempt_local707423720_0001_r_000000_0\n",
            "2023-01-07 16:27:40,668 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-01-07 16:27:40,669 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-01-07 16:27:40,669 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-01-07 16:27:40,676 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@d1bb8d9\n",
            "2023-01-07 16:27:40,679 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-01-07 16:27:40,710 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2023-01-07 16:27:40,713 INFO reduce.EventFetcher: attempt_local707423720_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2023-01-07 16:27:40,763 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local707423720_0001_m_000000_0 decomp: 1394 len: 1398 to MEMORY\n",
            "2023-01-07 16:27:40,768 INFO reduce.InMemoryMapOutput: Read 1394 bytes from map-output for attempt_local707423720_0001_m_000000_0\n",
            "2023-01-07 16:27:40,770 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1394, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1394\n",
            "2023-01-07 16:27:40,776 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2023-01-07 16:27:40,777 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-01-07 16:27:40,777 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2023-01-07 16:27:40,787 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-01-07 16:27:40,787 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1390 bytes\n",
            "2023-01-07 16:27:40,791 INFO reduce.MergeManagerImpl: Merged 1 segments, 1394 bytes to disk to satisfy reduce memory limit\n",
            "2023-01-07 16:27:40,792 INFO reduce.MergeManagerImpl: Merging 1 files, 1398 bytes from disk\n",
            "2023-01-07 16:27:40,793 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2023-01-07 16:27:40,793 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-01-07 16:27:40,795 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1390 bytes\n",
            "2023-01-07 16:27:40,796 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-01-07 16:27:40,809 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reducer.py]\n",
            "2023-01-07 16:27:40,815 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2023-01-07 16:27:40,818 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2023-01-07 16:27:40,859 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-01-07 16:27:40,860 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-01-07 16:27:40,862 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-01-07 16:27:40,963 INFO mapreduce.Job: Job job_local707423720_0001 running in uber mode : false\n",
            "2023-01-07 16:27:40,965 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-01-07 16:27:40,989 INFO streaming.PipeMapRed: Records R/W=143/1\n",
            "2023-01-07 16:27:40,995 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2023-01-07 16:27:40,996 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2023-01-07 16:27:40,997 INFO mapred.Task: Task:attempt_local707423720_0001_r_000000_0 is done. And is in the process of committing\n",
            "2023-01-07 16:27:40,998 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-01-07 16:27:40,998 INFO mapred.Task: Task attempt_local707423720_0001_r_000000_0 is allowed to commit now\n",
            "2023-01-07 16:27:41,001 INFO output.FileOutputCommitter: Saved output of task 'attempt_local707423720_0001_r_000000_0' to file:/content/output_hadoop\n",
            "2023-01-07 16:27:41,002 INFO mapred.LocalJobRunner: Records R/W=143/1 > reduce\n",
            "2023-01-07 16:27:41,002 INFO mapred.Task: Task 'attempt_local707423720_0001_r_000000_0' done.\n",
            "2023-01-07 16:27:41,003 INFO mapred.Task: Final Counters for attempt_local707423720_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=5934\n",
            "\t\tFILE: Number of bytes written=645257\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=85\n",
            "\t\tReduce shuffle bytes=1398\n",
            "\t\tReduce input records=143\n",
            "\t\tReduce output records=85\n",
            "\t\tSpilled Records=143\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=348127232\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=720\n",
            "2023-01-07 16:27:41,003 INFO mapred.LocalJobRunner: Finishing task: attempt_local707423720_0001_r_000000_0\n",
            "2023-01-07 16:27:41,003 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2023-01-07 16:27:41,966 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-01-07 16:27:41,967 INFO mapreduce.Job: Job job_local707423720_0001 completed successfully\n",
            "2023-01-07 16:27:41,981 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=9040\n",
            "\t\tFILE: Number of bytes written=1288396\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=21\n",
            "\t\tMap output records=143\n",
            "\t\tMap output bytes=1106\n",
            "\t\tMap output materialized bytes=1398\n",
            "\t\tInput split bytes=100\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=85\n",
            "\t\tReduce shuffle bytes=1398\n",
            "\t\tReduce input records=143\n",
            "\t\tReduce output records=85\n",
            "\t\tSpilled Records=286\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=41\n",
            "\t\tTotal committed heap usage (bytes)=696254464\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=900\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=720\n",
            "2023-01-07 16:27:41,981 INFO streaming.StreamJob: Output directory: /content/output_hadoop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Возможная ошибка 1. Права доступа\n",
        "#!chmod u+rwx /content/mapper.py\n",
        "#!chmod u+rwx /content/reducer.py"
      ],
      "metadata": {
        "id": "-jfRBNnECG1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Возможная ошибка 2. Путь к Python\n",
        "# !which python (результат /usr/local/bin/python)\n",
        "# В файлах mapper.py и reducer.py меняем /usr/bin/env на /usr/local/bin/python"
      ],
      "metadata": {
        "id": "qaN1O9PnGenq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Возможная ошибка 3. Кодировка файлов\n",
        "# В файлах mapper.py и reducer.py меняем Windows(CR LF) на Unix(LF) "
      ],
      "metadata": {
        "id": "7vVLv1rKHuCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -ls /content/output_hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OiAuywoIllg",
        "outputId": "5503a7b6-a46a-4716-bb9e-3dd73ba57144"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2023-01-07 16:27 /content/output_hadoop/_SUCCESS\n",
            "-rw-r--r--   1 root root        704 2023-01-07 16:27 /content/output_hadoop/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -cat /content/output_hadoop/part-00000 | head -85"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0ELtiqCIrz2",
        "outputId": "f8d57366-3e2e-45b4-e7ec-05bd33870029"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\t2\n",
            "although\t3\n",
            "ambiguity\t1\n",
            "and\t1\n",
            "are\t1\n",
            "arent\t1\n",
            "at\t1\n",
            "bad\t1\n",
            "be\t3\n",
            "beats\t1\n",
            "beautiful\t1\n",
            "better\t8\n",
            "break\t1\n",
            "by\t1\n",
            "cases\t1\n",
            "complex\t2\n",
            "complicated\t1\n",
            "counts\t1\n",
            "dense\t1\n",
            "do\t2\n",
            "dutch\t1\n",
            "easy\t1\n",
            "enough\t1\n",
            "errors\t1\n",
            "explain\t2\n",
            "explicit\t1\n",
            "explicitly\t1\n",
            "face\t1\n",
            "first\t1\n",
            "flat\t1\n",
            "good\t1\n",
            "great\t1\n",
            "guess\t1\n",
            "hard\t1\n",
            "honking\t1\n",
            "idea\t3\n",
            "if\t2\n",
            "implementation\t2\n",
            "implicit\t1\n",
            "in\t1\n",
            "is\t10\n",
            "it\t2\n",
            "its\t1\n",
            "lets\t1\n",
            "may\t2\n",
            "more\t1\n",
            "namespaces\t1\n",
            "nested\t1\n",
            "never\t3\n",
            "not\t1\n",
            "now\t2\n",
            "obvious\t2\n",
            "of\t3\n",
            "often\t1\n",
            "one\t3\n",
            "only\t1\n",
            "pass\t1\n",
            "peters\t1\n",
            "practicality\t1\n",
            "preferably\t1\n",
            "purity\t1\n",
            "python\t1\n",
            "readability\t1\n",
            "refuse\t1\n",
            "right\t1\n",
            "rules\t1\n",
            "should\t2\n",
            "silenced\t1\n",
            "silently\t1\n",
            "simple\t1\n",
            "sparse\t1\n",
            "special\t2\n",
            "temptation\t1\n",
            "than\t8\n",
            "that\t1\n",
            "the\t6\n",
            "there\t1\n",
            "those\t1\n",
            "tim\t1\n",
            "to\t5\n",
            "ugly\t1\n",
            "unless\t2\n",
            "way\t2\n",
            "youre\t1\n",
            "zen\t1\n"
          ]
        }
      ]
    }
  ]
}